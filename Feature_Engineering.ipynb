{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature Engineering.ipynb",
      "provenance": [],
      "mount_file_id": "1Cf159OhLfXThSmA-Xip_-rM0mgC6ACJm",
      "authorship_tag": "ABX9TyOqkzh2ldgzEFx92G8M5Dbi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Achraf-Trabelsi/PFA-NLP/blob/main/Feature_Engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1 adding features"
      ],
      "metadata": {
        "id": "6aokfuArNGq9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tx0d5K-gXDya",
        "outputId": "ab44293d-40c2-4312-fb6d-e3d8fcb697c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from textblob import TextBlob\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "train=pd.read_csv(\"/content/drive/MyDrive/PFA/train_pre\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "id": "OnoZpb1fn2vW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train=train.drop([\"id\",\"discourse_id\",\"discourse_type_num\",\"predictionstring\"],axis=1)"
      ],
      "metadata": {
        "id": "HLqsSxQSYZr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zt1A9-7JdYVR",
        "outputId": "8da5f0d2-f661-4460-b618-285c3f844ef5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_chars(text):\n",
        "    return len(text)"
      ],
      "metadata": {
        "id": "XODhHoEPY64U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_words(text):\n",
        "    return len(text.split())"
      ],
      "metadata": {
        "id": "dIi84F-PZqQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_capital_chars(text):\n",
        "    count=0\n",
        "    for i in text:\n",
        "        if i.isupper():\n",
        "            count+=1\n",
        "    return count"
      ],
      "metadata": {
        "id": "66ycJWbxZu83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_capital_words(text):\n",
        "    return sum(map(str.isupper,text.split()))"
      ],
      "metadata": {
        "id": "NVJcbTsrZylA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_punctuations(text):\n",
        "    punctuations=\"!$%(),.:;<>?@{}\"\n",
        "    count=0\n",
        "    for i in text:\n",
        "        if i in punctuations:\n",
        "          count+=1\n",
        "    return count"
      ],
      "metadata": {
        "id": "wy4q-H9VZ3K_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import regex as re\n",
        "def count_words_in_quotes(text):\n",
        "    x = re.findall(r'\"(.*?)\"' , text)\n",
        "    count=0\n",
        "    if x is None:\n",
        "        return 0\n",
        "    else:\n",
        "        for i in x:\n",
        "            t=i[1:-1]\n",
        "            count+=count_words(t)\n",
        "        return count"
      ],
      "metadata": {
        "id": "Ic5gBUD9aIrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk \n",
        "def count_sent(text):\n",
        "    return len(nltk.sent_tokenize(text))"
      ],
      "metadata": {
        "id": "WM59GWQ3aMg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_unique_words(text):\n",
        "    return len(set(text.split()))"
      ],
      "metadata": {
        "id": "yCO2Q8xPbXIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_polarity(text):\n",
        "    try:\n",
        "        textblob = TextBlob(text)\n",
        "        pol = textblob.sentiment.polarity\n",
        "    except:\n",
        "        pol = 0.0\n",
        "    return pol\n",
        "\n",
        "def get_subjectivity(text):\n",
        "    try:\n",
        "        textblob = TextBlob(text)\n",
        "        subj = textblob.sentiment.subjectivity\n",
        "    except:\n",
        "        subj = 0.0\n",
        "    return subj"
      ],
      "metadata": {
        "id": "rffuzF-RnpPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_dic = {\n",
        "    'noun' : ['NN','NNS','NNP','NNPS'],\n",
        "    'pron' : ['PRP','PRP$','WP','WP$'],\n",
        "    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
        "    'adj' :  ['JJ','JJR','JJS'],\n",
        "    'adv' : ['RB','RBR','RBS','WRB']\n",
        "}\n",
        "\n",
        "# function to check and get the part of speech tag count of a words in a given sentence\n",
        "def pos_check(x, flag):\n",
        "    cnt = 0\n",
        "    try:\n",
        "        wiki = TextBlob(x)\n",
        "        for tup in wiki.tags:\n",
        "            ppo = list(tup)[1]\n",
        "            if ppo in pos_dic[flag]:\n",
        "                cnt += 1\n",
        "    except:\n",
        "        pass\n",
        "    return cnt\n"
      ],
      "metadata": {
        "id": "NdvQBEGRrDm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['polarity'] = train['discourse_text'].apply(get_polarity)\n",
        "train['subjectivity'] = train['discourse_text'].apply(get_subjectivity)"
      ],
      "metadata": {
        "id": "wM8uBBjFp5uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['noun_count'] = train['discourse_text'].apply(lambda x: pos_check(x, 'noun'))\n",
        "train['verb_count'] = train['discourse_text'].apply(lambda x: pos_check(x, 'verb'))\n",
        "train['adj_count'] = train['discourse_text'].apply(lambda x: pos_check(x, 'adj'))\n",
        "train['adv_count'] = train['discourse_text'].apply(lambda x: pos_check(x, 'adv'))\n",
        "train['pron_count'] = train['discourse_text'].apply(lambda x: pos_check(x, 'pron'))"
      ],
      "metadata": {
        "id": "tJ4b1huErChJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['char_count'] = train[\"discourse_text\"].apply(lambda x:count_chars(x))\n",
        "train['word_count'] = train[\"discourse_text\"].apply(lambda x:count_words(x))\n",
        "train['sent_count'] = train[\"discourse_text\"].apply(lambda x:count_sent(x))\n",
        "train['capital_char_count'] = train[\"discourse_text\"].apply(lambda x:count_capital_chars(x))\n",
        "train['unique_word_count'] = train[\"discourse_text\"].apply(lambda x:count_unique_words(x))\n",
        "train['avg_wordlength'] = train['char_count']/train['word_count']\n",
        "train['avg_sentlength'] = train['word_count']/train['sent_count']\n",
        "train['unique_vs_words'] = train['unique_word_count']/train['word_count']"
      ],
      "metadata": {
        "id": "zK3OaYNGb51L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 583
        },
        "id": "lUNYKch-qHIK",
        "outputId": "3af4d8c6-ed3c-4e2a-c41d-a1d42232d2c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   discourse_start  discourse_end  \\\n",
              "0              8.0          229.0   \n",
              "1            230.0          312.0   \n",
              "2            313.0          401.0   \n",
              "3            402.0          758.0   \n",
              "4            759.0          886.0   \n",
              "\n",
              "                                      discourse_text discourse_type  polarity  \\\n",
              "0  Modern humans today are always on their phone....           Lead  0.183333   \n",
              "1  They are some really bad consequences when stu...       Position -0.700000   \n",
              "2  Some certain areas in the United States ban ph...       Evidence  0.214286   \n",
              "3  When people have phones, they know about certa...       Evidence  0.029762   \n",
              "4  Driving is one of the way how to get around. P...          Claim -0.333333   \n",
              "\n",
              "   subjectivity  noun_count  verb_count  adj_count  adv_count  pron_count  \\\n",
              "0      0.216667          11           8          3          6           6   \n",
              "1      0.666667           3           3          1          3           2   \n",
              "2      0.571429           6           1          1          1           1   \n",
              "3      0.515476          17          13          4          6           5   \n",
              "4      0.666667           4           6          1          2           2   \n",
              "\n",
              "   char_count  word_count  sent_count  capital_char_count  unique_word_count  \\\n",
              "0         221          44           3                   5                 36   \n",
              "1          82          15           1                   1                 14   \n",
              "2          88          16           1                   3                 16   \n",
              "3         356          63           4                   9                 50   \n",
              "4         127          24           3                   4                 24   \n",
              "\n",
              "   avg_wordlength  avg_sentlength  unique_vs_words  \\\n",
              "0        5.022727       14.666667         0.818182   \n",
              "1        5.466667       15.000000         0.933333   \n",
              "2        5.500000       16.000000         1.000000   \n",
              "3        5.650794       15.750000         0.793651   \n",
              "4        5.291667        8.000000         1.000000   \n",
              "\n",
              "                                          text_clean  \n",
              "0  modern human today are always on their phone t...  \n",
              "1  they are some really bad consequence when stuf...  \n",
              "2  some certain area in the united state ban phon...  \n",
              "3  when people have phone they know about certain...  \n",
              "4  driving is one of the way how to get around pe...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d1acf85e-5eab-40d6-acdf-7f322bd73723\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>discourse_start</th>\n",
              "      <th>discourse_end</th>\n",
              "      <th>discourse_text</th>\n",
              "      <th>discourse_type</th>\n",
              "      <th>polarity</th>\n",
              "      <th>subjectivity</th>\n",
              "      <th>noun_count</th>\n",
              "      <th>verb_count</th>\n",
              "      <th>adj_count</th>\n",
              "      <th>adv_count</th>\n",
              "      <th>pron_count</th>\n",
              "      <th>char_count</th>\n",
              "      <th>word_count</th>\n",
              "      <th>sent_count</th>\n",
              "      <th>capital_char_count</th>\n",
              "      <th>unique_word_count</th>\n",
              "      <th>avg_wordlength</th>\n",
              "      <th>avg_sentlength</th>\n",
              "      <th>unique_vs_words</th>\n",
              "      <th>text_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>Modern humans today are always on their phone....</td>\n",
              "      <td>Lead</td>\n",
              "      <td>0.183333</td>\n",
              "      <td>0.216667</td>\n",
              "      <td>11</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>221</td>\n",
              "      <td>44</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>36</td>\n",
              "      <td>5.022727</td>\n",
              "      <td>14.666667</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>modern human today are always on their phone t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>230.0</td>\n",
              "      <td>312.0</td>\n",
              "      <td>They are some really bad consequences when stu...</td>\n",
              "      <td>Position</td>\n",
              "      <td>-0.700000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>82</td>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>5.466667</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>0.933333</td>\n",
              "      <td>they are some really bad consequence when stuf...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>313.0</td>\n",
              "      <td>401.0</td>\n",
              "      <td>Some certain areas in the United States ban ph...</td>\n",
              "      <td>Evidence</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>88</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>16</td>\n",
              "      <td>5.500000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>some certain area in the united state ban phon...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>402.0</td>\n",
              "      <td>758.0</td>\n",
              "      <td>When people have phones, they know about certa...</td>\n",
              "      <td>Evidence</td>\n",
              "      <td>0.029762</td>\n",
              "      <td>0.515476</td>\n",
              "      <td>17</td>\n",
              "      <td>13</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>356</td>\n",
              "      <td>63</td>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>50</td>\n",
              "      <td>5.650794</td>\n",
              "      <td>15.750000</td>\n",
              "      <td>0.793651</td>\n",
              "      <td>when people have phone they know about certain...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>759.0</td>\n",
              "      <td>886.0</td>\n",
              "      <td>Driving is one of the way how to get around. P...</td>\n",
              "      <td>Claim</td>\n",
              "      <td>-0.333333</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>127</td>\n",
              "      <td>24</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>24</td>\n",
              "      <td>5.291667</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>driving is one of the way how to get around pe...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d1acf85e-5eab-40d6-acdf-7f322bd73723')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d1acf85e-5eab-40d6-acdf-7f322bd73723 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d1acf85e-5eab-40d6-acdf-7f322bd73723');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2 text cleaning"
      ],
      "metadata": {
        "id": "TEXqHeRxNbSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lst_stopwords = nltk.corpus.stopwords.words(\"english\")\n",
        "other_words_to_take_out = ['school', 'students', 'people', 'would', 'could', 'many','coul','shoul','woul','stuent']\n",
        "lst_stopwords=lst_stopwords.extend(other_words_to_take_out)"
      ],
      "metadata": {
        "id": "bmpohVDHlcYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ps = nltk.stem.porter.PorterStemmer()\n",
        "lem = nltk.stem.wordnet.WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "vBnc39hWltS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "porter = PorterStemmer()\n",
        "lancaster=LancasterStemmer()\n",
        "def utils_preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):\n",
        "    ##Clean (convert to lowercase and delete punctuation and characters, then delete)\n",
        "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
        "            \n",
        "    ##Tokenization (conversion from string to list)\n",
        "    lst_text = text.split()\n",
        "    ##Delete stop words\n",
        "    if lst_stopwords is not None:\n",
        "        lst_text = [word for word in lst_text if word not in \n",
        "                    lst_stopwords]\n",
        "                \n",
        "    ##Stemming\n",
        "    if flg_stemm == True:\n",
        "        ps = nltk.stem.porter.PorterStemmer()\n",
        "        lst_text = [ps.stem(word) for word in lst_text]\n",
        "                \n",
        "    ##Lemmatization\n",
        "    if flg_lemm == True:\n",
        "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "        lst_text = [lem.lemmatize(word) for word in lst_text]\n",
        "            \n",
        "    ##Return to string from list\n",
        "    text = \" \".join(lst_text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "a7OVpbWGjcMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[\"text_clean\"] = train[\"discourse_text\"].apply(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=True, lst_stopwords=lst_stopwords))\n"
      ],
      "metadata": {
        "id": "BZqGO5M7jtCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.describe()"
      ],
      "metadata": {
        "id": "mYcyAGwXqHYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3 making the model"
      ],
      "metadata": {
        "id": "Nv-_QQ0uNvte"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TF IDF**"
      ],
      "metadata": {
        "id": "xrfTs1QFo5UL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer=  TfidfVectorizer(max_features=250)\n",
        "train_tf_idf_features =  vectorizer.fit_transform(train['text_clean']).toarray()"
      ],
      "metadata": {
        "id": "yT42ylHfo1GP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tf_idf = pd.DataFrame(train_tf_idf_features)"
      ],
      "metadata": {
        "id": "Odfyj_FEuktG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_idf= pd.merge(train_tf_idf,train,left_index=True, right_index=True)"
      ],
      "metadata": {
        "id": "Holhc-j1v_0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target=df_idf[\"discourse_type\"]\n",
        "df_idf=df_idf.drop([\"discourse_type\",\"discourse_text\",\"text_clean\"],axis=1)\n"
      ],
      "metadata": {
        "id": "jM_hW2JuwZ8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels=['Lead', 'Position', 'Evidence', 'Claim', 'Concluding Statement','Counterclaim', 'Rebuttal']\n",
        "\n",
        "target=target.apply(labels.index)"
      ],
      "metadata": {
        "id": "8duZRxFBxmlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "def split(X,y,size):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=size, random_state=42,stratify=y )\n",
        "  return  X_train, X_test, y_train, y_test\n"
      ],
      "metadata": {
        "id": "pn5pA6HNydBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = split(df_idf,target,0.2)"
      ],
      "metadata": {
        "id": "C3cxjFBGF0vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**statistical model training**"
      ],
      "metadata": {
        "id": "nXHe8bhfyv7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "VUUintqGUZdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf_model = RandomForestClassifier(n_estimators = 150, min_samples_split = 15, random_state = 42)\n",
        "clf_model.fit(X_train, y_train)\n",
        "y_pred = clf_model.predict(X_test)\n",
        "accuracy_score(y_test,y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jc8yawBFy1vy",
        "outputId": "670c4aa3-5efc-4883-af63-1748d8841556"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7306905991198587"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cm=confusion_matrix(y_test,y_pred)\n",
        "acc_per_class={}\n",
        "for i in range(len(cm)):\n",
        "  TP=cm[i,i]\n",
        "  TN=cm.sum()-cm[i,:].sum()-cm[:,i].sum()\n",
        "  ACC=(TP+TN)/cm.sum()\n",
        "  acc_per_class[labels[i]]=ACC"
      ],
      "metadata": {
        "id": "JIAriSUH1zSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc_per_class"
      ],
      "metadata": {
        "id": "cjBB9bpB2pcB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b41f7e06-bd9b-4288-82a8-0bb074922cc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Claim': 0.4977996465573998,\n",
              " 'Concluding Statement': 0.8971551335805121,\n",
              " 'Counterclaim': 0.9589036349145847,\n",
              " 'Evidence': 0.6011989327419522,\n",
              " 'Lead': 0.9268165910114695,\n",
              " 'Position': 0.8797602134516096,\n",
              " 'Rebuttal': 0.9690564468623306}"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_pred, target_names=labels))"
      ],
      "metadata": {
        "id": "ErkwOGLO2Cps",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "572e4e84-fda4-4fe7-8b41-c1d55e3a70bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                      precision    recall  f1-score   support\n",
            "\n",
            "                Lead       0.87      0.87      0.87      1861\n",
            "            Position       0.80      0.51      0.62      3084\n",
            "            Evidence       0.76      0.81      0.79      9141\n",
            "               Claim       0.67      0.91      0.78     10042\n",
            "Concluding Statement       0.79      0.37      0.51      2701\n",
            "        Counterclaim       0.88      0.14      0.24      1163\n",
            "            Rebuttal       0.81      0.12      0.22       867\n",
            "\n",
            "            accuracy                           0.73     28859\n",
            "           macro avg       0.80      0.53      0.57     28859\n",
            "        weighted avg       0.75      0.73      0.70     28859\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf = LogisticRegression('l2',max_iter=750)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy_score(y_test,y_pred)"
      ],
      "metadata": {
        "id": "mNKggUkt0U0E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b08c7f0-145b-4653-8d2d-3801d07ddb30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6512699677743512"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deep learning models**"
      ],
      "metadata": {
        "id": "VXZ96KurOfZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.regularizers import l2\n",
        "def create_deep_model(factor, rate):\n",
        "    model = Sequential()     \n",
        "    model.add(Dense(units=1024,kernel_regularizer=l2(factor), \n",
        "      activation='relu')), Dropout(rate),\n",
        "    model.add(Dense(units=1024,kernel_regularizer=l2(factor),\n",
        "      activation='relu')), Dropout(rate),\n",
        "    model.add(Dense(units=512,kernel_regularizer=l2(factor),\n",
        "      activation='relu')), Dropout(rate),\n",
        "    #Output layer\n",
        "    model.add(Dense(units=7, activation='softmax'))\n",
        "    return model"
      ],
      "metadata": {
        "id": "Fo6VZe3C5S_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=create_deep_model(0.001,0.25)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam',  metrics=['acc']) "
      ],
      "metadata": {
        "id": "0H3cFYDG5aSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = pd.get_dummies(target)"
      ],
      "metadata": {
        "id": "3UR_CJGK72ln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_idf,y,test_size=0.2, random_state=42,stratify=target )"
      ],
      "metadata": {
        "id": "V519T9hH7g8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=model.fit(x=X_train, y=y_train, batch_size=256, epochs=20, validation_data=(X_test, y_test), verbose=1)"
      ],
      "metadata": {
        "id": "Bf3b6rGV5io0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pretrained models**"
      ],
      "metadata": {
        "id": "UDGbJoPOEu-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re \n",
        "from numpy import array,asarray ,zeros\n",
        "from keras.preprocessing.text import one_hot \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential,Model \n",
        "from keras.layers.core import Activation, Dropout, Dense \n",
        "from keras.layers import Flatten,Conv1D, GlobalMaxPooling1D,MaxPooling1D,LSTM,GRU\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "8pZS2-pkE2oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=[i for i in train[\"text_clean\"]]"
      ],
      "metadata": {
        "id": "xYhWTiVCFKcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = split(X, y,0.33)"
      ],
      "metadata": {
        "id": "VRQCtudrFe-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=500) \n",
        "tokenizer.fit_on_texts(X_train) \n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)"
      ],
      "metadata": {
        "id": "Tv_3ZYmuGikm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1 \n",
        "maxlen = 150\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen) \n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
      ],
      "metadata": {
        "id": "HcW0anPzGn8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embeddings(path):\n",
        "    embeddings_dictionary = dict() \n",
        "    embeddings_file = open(path) \n",
        "    for line in embeddings_file:\n",
        "        records = line.split()\n",
        "        word = records[0] \n",
        "        vector_dimensions = asarray(records[1:],dtype='float32') \n",
        "        embeddings_dictionary[word] = vector_dimensions\n",
        "    embeddings_file.close()\n",
        "    return embeddings_dictionary"
      ],
      "metadata": {
        "id": "WlIqdaIeGx0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_embedding_matrix(emb,tokenizer):\n",
        "  embedding_matrix = zeros((vocab_size, 300)) \n",
        "  for word, index in tokenizer.word_index.items(): \n",
        "      embedding_vector = emb.get(word) \n",
        "      if embedding_vector is not None: \n",
        "          embedding_matrix[index] = embedding_vector\n",
        "  return embedding_matrix"
      ],
      "metadata": {
        "id": "cxf0zj9eJcNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d qiushiwang/original-embeddings\n",
        "!kaggle datasets download -d vsmolyakov/fasttext\n",
        "!unzip /content/original-embeddings.zip\n",
        "!unzip /content/fasttext.zip"
      ],
      "metadata": {
        "id": "UO3nl8VTG5O-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f7c4c26-a3e6-4a3d-cfe3-6ef306818606"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.64.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.1.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.10.8)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "Downloading original-embeddings.zip to /content\n",
            " 98% 750M/765M [00:27<00:00, 26.6MB/s]\n",
            "100% 765M/765M [00:27<00:00, 29.0MB/s]\n",
            "Downloading fasttext.zip to /content\n",
            " 91% 97.0M/107M [00:04<00:00, 22.6MB/s]\n",
            "100% 107M/107M [00:04<00:00, 24.3MB/s] \n",
            "Archive:  /content/original-embeddings.zip\n",
            "  inflating: GoogleNews-vectors-negative300(first500000).txt  \n",
            "  inflating: glove.6B.300d.txt       \n",
            "Archive:  /content/fasttext.zip\n",
            "  inflating: wiki.simple.vec         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def model_cnn(X_train,y_train,l,embedding_matrix):\n",
        "   \n",
        "    model_cnn=Sequential()\n",
        "    model_cnn.add(Embedding(vocab_size, l, weights=[embedding_matrix],input_length=maxlen))\n",
        "    model_cnn.add(Conv1D(128,5,activation='relu'))\n",
        "    model_cnn.add(GlobalMaxPooling1D())\n",
        "    model_cnn.add(Dense(7,activation='softmax'))\n",
        "    model_cnn.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
        "    history=model_cnn.fit(X_train,y_train,epochs=10,batch_size=128,verbose=0,validation_split=0.2)\n",
        "    score = model_cnn.evaluate(X_test, y_test, verbose=1)\n",
        "    print(\"Test Score:\", score[0]) \n",
        "    print(\"Test Accuracy:\", score[1])\n",
        "    return model_cnn,history"
      ],
      "metadata": {
        "id": "7DY57ZAPEe31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_RNN(X_train,y_train,l,embedding_matrix):\n",
        "    model_rnn=Sequential()\n",
        "    model_rnn.add(Embedding(vocab_size, l, weights=[embedding_matrix],input_length=maxlen))\n",
        "    model_rnn.add(LSTM(128))\n",
        "    model_rnn.add(Dense(256,activation='relu'))\n",
        "    model_rnn.add(Dense(7,activation='softmax'))\n",
        "    model_rnn.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
        "    history=model_rnn.fit(X_train,y_train,epochs=10,batch_size=128,verbose=0,validation_split=0.2)\n",
        "    score = model_rnn.evaluate(X_test, y_test, verbose=1)\n",
        "    print(\"Test Score:\", score[0]) \n",
        "    print(\"Test Accuracy:\", score[1])\n",
        "    return model_rnn,history"
      ],
      "metadata": {
        "id": "P_M3x_n5KeXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_RNNCNN(X_train,y_train,l,embedding_matrix):\n",
        "    modelx = Sequential()\n",
        "    modelx.add((Embedding(vocab_size,l, weights=[embedding_matrix],input_length=maxlen)))\n",
        "    modelx.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "    modelx.add(MaxPooling1D(pool_size=2))\n",
        "    modelx.add(LSTM(128))\n",
        "    modelx.add(Dense(7, activation='softmax'))\n",
        "    modelx.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    history=modelx.fit(X_train,y_train,epochs=10,batch_size=128,verbose=0,validation_split=0.2)\n",
        "    score = modelx.evaluate(X_test, y_test, verbose=1)\n",
        "    print(\"Test Score:\", score[0]) \n",
        "    print(\"Test Accuracy:\", score[1])\n",
        "    return modelx,history"
      ],
      "metadata": {
        "id": "L8XaFoxVLxWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "GLOVE\n",
        "\n"
      ],
      "metadata": {
        "id": "8AUiBRWiVSF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "glove=embeddings(\"/content/glove.6B.300d.txt\")"
      ],
      "metadata": {
        "id": "kwZ2MK3VG19t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove_matrix=make_embedding_matrix(glove,tokenizer)"
      ],
      "metadata": {
        "id": "Zq1qn2iuLPyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model,history=model_cnn(X_train,y_train,300,glove_matrix)"
      ],
      "metadata": {
        "id": "aCEhnNZbJqyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_rnn,history=model_RNN(X_train,y_train,300,glove_matrix)"
      ],
      "metadata": {
        "id": "kpsAazjxLn1o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9df60559-652c-4e4d-a0a1-c2374a8ba2a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1489/1489 [==============================] - 21s 14ms/step - loss: 0.8230 - acc: 0.7190\n",
            "Test Score: 0.8229736089706421\n",
            "Test Accuracy: 0.719028890132904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_rnncnn,hist=model_RNNCNN(X_train,y_train,300,glove_matrix)"
      ],
      "metadata": {
        "id": "1yOprM7dMbDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "Word2Vec\n"
      ],
      "metadata": {
        "id": "66y3WYLVVYKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec=embeddings(\"/content/GoogleNews-vectors-negative300(first500000).txt\")"
      ],
      "metadata": {
        "id": "oabFjO5uMsD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_matrix=make_embedding_matrix(word2vec,tokenizer)"
      ],
      "metadata": {
        "id": "ZCDFtEaNPJ3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1,history=model_cnn(X_train,y_train,300,word2vec_matrix)"
      ],
      "metadata": {
        "id": "WCxKAi2cPR--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_rnn1,history=model_RNN(X_train,y_train,300,word2vec_matrix)"
      ],
      "metadata": {
        "id": "Hoc5XUsCPe1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_rnncnn1,hist=model_RNNCNN(X_train,y_train,300,glove_matrix)"
      ],
      "metadata": {
        "id": "ORPYbaujVh0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "FASTTEXT"
      ],
      "metadata": {
        "id": "3iQi9XqyYQyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import os, re, csv, math, codecs"
      ],
      "metadata": {
        "id": "WpxG8hf0Vjmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fasttext = {}\n",
        "f = codecs.open('/content/wiki.simple.vec', encoding='utf-8')\n",
        "for line in tqdm(f):\n",
        "    values = line.rstrip().rsplit(' ')\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    fasttext[word] = coefs\n",
        "f.close()\n",
        "print('found %s word vectors' % len(fasttext))"
      ],
      "metadata": {
        "id": "Bi_1sxdSVzhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fasttext_matrix = zeros((vocab_size,300)) \n",
        "for word, index in tokenizer.word_index.items(): \n",
        "    embeddings_ind = fasttext.get(word) \n",
        "    if embeddings_ind is not None: \n",
        "        fasttext_matrix[index] = embeddings_ind"
      ],
      "metadata": {
        "id": "_G-N2BTxV-Pb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_cnn2,hist=model_cnn(X_train,y_train,300,fasttext_matrix)"
      ],
      "metadata": {
        "id": "h888if2UWhVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_rnn2,hist=model_RNN(X_train,y_train,300,fasttext_matrix)"
      ],
      "metadata": {
        "id": "KXDH-e2-Wh-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_rnncnn2,hist=model_RNNCNN(X_train,y_train,300,fasttext_matrix)"
      ],
      "metadata": {
        "id": "o2YoQACyWjHa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}